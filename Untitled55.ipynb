{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "   LSA NNMF\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "0  NaN  NaN\n",
      "Topic 1:\n",
      "   LSA NNMF\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "1  NaN  NaN\n",
      "Topic 2:\n",
      "   LSA NNMF\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "2  NaN  NaN\n",
      "Topic 3:\n",
      "   LSA NNMF\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "3  NaN  NaN\n",
      "Topic 4:\n",
      "   LSA NNMF\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "4  NaN  NaN\n",
      "Topic 5:\n",
      "   LSA NNMF\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "5  NaN  NaN\n",
      "Topic 6:\n",
      "   LSA NNMF\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "6  NaN  NaN\n",
      "Topic 7:\n",
      "   LSA NNMF\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n",
      "7  NaN  NaN\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Users\\rabia\\Desktop\\Thinkful\\rabia.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#newsgroups = fetch_20newsgroups()\n",
    "\n",
    "#from pprint import pprint\n",
    "#pprint(list(newsgroups.target_names))\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#reading in the data, this time in the form of paragraphs\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#newsgroups = fetch_20newsgroups()\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categs =['alt.atheism',\n",
    "         'rec.autos',\n",
    "         'sci.electronics',\n",
    "         'sci.med',\n",
    "         'sci.space',\n",
    "         'soc.religion.christian',\n",
    "         'talk.politics.guns',\n",
    "         'talk.politics.mideast']\n",
    "\n",
    "\n",
    "news_train = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), categories = categs)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "news_train_tfidf = vectorizer.fit_transform(news_train.data)\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "#print (news_train_tfidf[:1] )\n",
    "#print news_train_tfidf.shape\n",
    "\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    words_by_topic=tfidf.T * solution\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        #chosenlist=chosen.index +\" \"+round(chosen,2)#.map(str) \n",
    "        chosenlist=chosen.index +\" \"+chosen.map(str) \n",
    "        topwords.loc[column]=chosenlist\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10\n",
    "ntopics=8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "news_train_lsa = lsa.fit_transform(news_train_tfidf)\n",
    "\n",
    "components_lsa = word_topic(news_train_tfidf, news_train_lsa, vocabulary)\n",
    "\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)\n",
    "# working\n",
    "#print topwords['LSA']\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "news_train_nmf = nmf.fit_transform(news_train_tfidf) \n",
    "\n",
    "components_nmf = word_topic(news_train_tfidf, news_train_nmf, vocabulary)\n",
    "\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)\n",
    "\n",
    "#print topwords['NNMF']\n",
    "\n",
    "\n",
    "\n",
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
